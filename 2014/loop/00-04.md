Simulation Testing (Michael Nygard)
===================================

1. Axes of Testing
  - example-based vs property-based
  - white box vs black box
  - automated vs manual

2. Example-Based Testing
  1. programmer probes the code
  2. programmer creates test cases
  3. progammer validates that expected outcome is actual outcome
  {example based test in a lisp}

  - scales: unit, functiona, acceptance
  - styles: test-after, test-driven, behavior-driven
  - common idioms: fixtures, stubs, mocks

  - Examples are great for a _lot_ of things, but they have some weaknesses in determining how a system will behave at runtime.
    - limited coverage - we only test the things that we add test cases for. there are often a range of other things (e.g. if we're testing addition, we are only testing the numbers that we use for the tests)
    - fragility: what about when the tests become fragile? (e.g. having to change the test code to support changes to the code-under-test
    - poor scalability
    - non-composability: unit tests can only test small pieces of code. what happens when we combine those pieces? we often have bugs in those places, but can't spot them unless we write new, explicit tests.
 
 3. Property-based testing (simulation testing is a subset of property-based testing)
  - programmer models the domain and invariants
  - a _program_ generates many individual tests
  - validates categoric properties, not specific outcomes
  - program can find failing case, then present the dev with the minimal surface area that is producing the bug

4. Simulation Testing
  - on original axes, it's "property-based, white-box, automated testing"
  - extended property based testing to whole systems:
    - programmer models domain
    - program generates repeatable script of inputs
    - system tested in-situ â€“ the real, running system. this introduces some non-determinism. how do we avoid this NDT from invalidating our results?
    - validate categoric and global properties repeatedly.
      - this allows validating global properties (e.g. did all of my API calls return ok? did pages all respond within response time? we can't ask these questions while executing.)

5. What is testing?
  - attempts to prove the system is wrong
  - not an attempt to prove that the system is right
  - search problem: find interactions that result in forbidden zone of state space (...what the hell does this mean? ok, I get it)

  coverage:
    - code coverage is misleading.
    - state space coverage is better. we want to know that we're only allowing allowable states.
    - we must put bounds on test execution time, but we want the greatest confidence for that time.
      - this means exercising many, many trajectories, but bound to the amount of time we're willing to spend executing.
      - so, focus on state coverage, not code coverage.
 
6. so many moving parts. really.
  - decoupling via immutable databases
  - repeatable tests
  - repeatable verifications

  parts:
    - activity model: "I specify how I expect inputs to system to behave"b
    - setup:
      - initialize target system
    - execution:
      - runs purely on a script
      - simulation is run
    - output capture
      - happens simultaneous with execution
    - validation (...)
    - reporting


7. Simulant: an open source framework for simulation testing
  - model: a small number of parameters relative to the nature of the system
    - e.g. a user interacting with system goes through {x} states
    - tests are broken up into event streams per-user
    - stream output is written to test results
    - model is fed into a generator that is fed an unlimited amount of activity
      - this generates a bunch of different state names that result in data structures for all user states
  - simulation runner: sets up system, executes activities, captures output, generates reports
  - actions: move test stuff into actions so that when something in the system changes, we can change that action instead of changing many, many tests. 
    - containing non-determinism is difficult, but worth it.
  - build test record
  - validation: verify properties that are specific to actions and results, and global properties
    - speaker says validations are really queries

 8. Advantages:
  - coverage. delivers more exploration than scripted tests. speaker was able to find more bugs than QA.
  - (... more)


9. Considerations
  - what kind of models suffices?
  - containing randomness in generator. gotta curb this.
  - repeatability is sometimes challenging.
  - target system setup
  - test duration tweaking (how short - how many do we generate?)

10. Speaker says: "Try Simulant" - for one, the cost of maintenance is way lower.
